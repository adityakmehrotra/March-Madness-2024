{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 8.276405903937013\n",
      "Mean Absolute Error (MAE): 6.565912547528519\n",
      "R-squared (R²): 0.31440906276052855\n",
      "Predicted Team 1 Score: 79.87, Predicted Team 2 Score: 69.47\n",
      "Actual Team 1 Score: 79, Actual Team 2 Score: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sample data loading\n",
    "# Replace 'your_dataset.csv' with the path to your dataset\n",
    "# Assume columns like Team1Stat1, Team1Stat2, ..., Team2Stat1, Team2Stat2, ..., Team1Score, Team2Score\n",
    "df = pd.read_csv('all_meta_data_id.csv')\n",
    "\n",
    "\n",
    "# Prepare features and labels\n",
    "# Assuming the last two columns are Team1Score and Team2Score\n",
    "# C, D, X, Y, Z, AC, AD, AX, AY, AZ\n",
    "# 2, 3, 23, 24, 25, 28, 29, 49, 50, 51\n",
    "X = df.drop(df.columns[[2, 3, 23, 24, 25, 28, 29, 49, 50, 51]], axis=1)\n",
    "\n",
    "# Z, AZ\n",
    "# 25, 51\n",
    "y = df.iloc[:, [25, 51]]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Calculate R-squared (R²)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f'R-squared (R²): {r_squared}')\n",
    "\n",
    "# Select a \"random\" row from the dataset to simulate an unseen game prediction\n",
    "# For demonstration, let's take the first row of the test set\n",
    "unseen_game_stats = X_test.iloc[0].to_numpy().reshape(1, -1)\n",
    "predicted_scores = model.predict(unseen_game_stats)\n",
    "\n",
    "# Print the predicted scores\n",
    "print(f'Predicted Team 1 Score: {predicted_scores[0][0]}, Predicted Team 2 Score: {predicted_scores[0][1]}')\n",
    "\n",
    "# Optionally, compare with the actual scores\n",
    "actual_scores = y_test.iloc[0].to_numpy()\n",
    "print(f'Actual Team 1 Score: {actual_scores[0]}, Actual Team 2 Score: {actual_scores[1]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Predicted Team 1 Score  Predicted Team 2 Score\n",
      "0                    67.22                   58.02\n",
      "1                    80.39                   72.99\n",
      "2                    91.81                   71.47\n",
      "3                    81.49                   67.06\n",
      "4                    94.44                   73.31\n",
      "..                     ...                     ...\n",
      "62                   82.19                   72.77\n",
      "63                   60.98                   58.31\n",
      "64                   75.97                   62.72\n",
      "65                   71.26                   67.05\n",
      "66                   75.43                   60.56\n",
      "\n",
      "[67 rows x 2 columns]\n",
      "Team 1 Team Name\n",
      "Team 2 Team Name\n",
      "Predicted Team 1 Score\n",
      "Predicted Team 2 Score\n",
      "HI11\n",
      "Team 1 Team Name\n",
      "Team 2 Team Name\n",
      "Predicted Team 1 Score\n",
      "Predicted Team 2 Score\n",
      "99\n",
      "TEMP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Placeholder - you need actual scores for this part\\n# predictions_with_teams[\\'Actual Team 1 Score\\'] = [actual_scores_from_somewhere]\\n# predictions_with_teams[\\'Actual Team 2 Score\\'] = [actual_scores_from_somewhere]\\n\\n# Let\\'s say we define a correct prediction as being within 10 points of the actual score\\n# Calculate the error for each team\\'s score prediction\\npredictions_with_teams[\\'Team 1 Error\\'] = abs(predictions_with_teams[\\'Predicted Team 1 Score\\'] - predictions_with_teams[\\'Actual Team 1 Score\\'])\\npredictions_with_teams[\\'Team 2 Error\\'] = abs(predictions_with_teams[\\'Predicted Team 2 Score\\'] - predictions_with_teams[\\'Actual Team 2 Score\\'])\\n\\n# Define a correct prediction as having an error of 10 points or fewer\\naccuracy_threshold = 10\\npredictions_with_teams[\\'Team 1 Accurate\\'] = predictions_with_teams[\\'Team 1 Error\\'] <= accuracy_threshold\\npredictions_with_teams[\\'Team 2 Accurate\\'] = predictions_with_teams[\\'Team 2 Error\\'] <= accuracy_threshold\\n\\n# Calculate the percentage of accurate predictions\\nteam_1_accuracy_percentage = predictions_with_teams[\\'Team 1 Accurate\\'].mean() * 100\\nteam_2_accuracy_percentage = predictions_with_teams[\\'Team 2 Accurate\\'].mean() * 100\\n\\nprint(f\"Team 1 Prediction Accuracy: {team_1_accuracy_percentage}%\")\\nprint(f\"Team 2 Prediction Accuracy: {team_2_accuracy_percentage}%\")\\n\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming the first two columns are 'Team1Name' and 'Team2Name'\n",
    "\n",
    "file_path = 'pred_data.csv'  # Update with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "team_names = data.iloc[:, [2, 24]]\n",
    "features = data.drop(data.columns[[2, 24]], axis= 1)\n",
    "\n",
    "predicted_scores = model.predict(features)\n",
    "\n",
    "# Create a DataFrame for easier visualization of predictions with team names\n",
    "predictions = pd.DataFrame(predicted_scores, columns=['Predicted Team 1 Score', 'Predicted Team 2 Score'])\n",
    "print(predictions)\n",
    "'''\n",
    "predictions.to_csv('predicted.csv', index=False)\n",
    "predictions_with_teams = pd.concat([team_names, predictions], axis=1)\n",
    "rows = []\n",
    "rows.append(predictions_with_teams)\n",
    "for row in predictions_with_teams:\n",
    "    print(row)\n",
    "    \n",
    "print(\"HI11\")\n",
    "\n",
    "for youx in predictions_with_teams:\n",
    "    print(youx)\n",
    "\n",
    "print(\"99\")\n",
    "print(\"TEMP\")\n",
    "\n",
    "# Placeholder - you need actual scores for this part\n",
    "# predictions_with_teams['Actual Team 1 Score'] = [actual_scores_from_somewhere]\n",
    "# predictions_with_teams['Actual Team 2 Score'] = [actual_scores_from_somewhere]\n",
    "\n",
    "# Let's say we define a correct prediction as being within 10 points of the actual score\n",
    "# Calculate the error for each team's score prediction\n",
    "predictions_with_teams['Team 1 Error'] = abs(predictions_with_teams['Predicted Team 1 Score'] - predictions_with_teams['Actual Team 1 Score'])\n",
    "predictions_with_teams['Team 2 Error'] = abs(predictions_with_teams['Predicted Team 2 Score'] - predictions_with_teams['Actual Team 2 Score'])\n",
    "\n",
    "# Define a correct prediction as having an error of 10 points or fewer\n",
    "accuracy_threshold = 10\n",
    "predictions_with_teams['Team 1 Accurate'] = predictions_with_teams['Team 1 Error'] <= accuracy_threshold\n",
    "predictions_with_teams['Team 2 Accurate'] = predictions_with_teams['Team 2 Error'] <= accuracy_threshold\n",
    "\n",
    "# Calculate the percentage of accurate predictions\n",
    "team_1_accuracy_percentage = predictions_with_teams['Team 1 Accurate'].mean() * 100\n",
    "team_2_accuracy_percentage = predictions_with_teams['Team 2 Accurate'].mean() * 100\n",
    "\n",
    "print(f\"Team 1 Prediction Accuracy: {team_1_accuracy_percentage}%\")\n",
    "print(f\"Team 2 Prediction Accuracy: {team_2_accuracy_percentage}%\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team 1 Prediction Accuracy: 3.193916349809886%\n",
      "Team 2 Prediction Accuracy: 3.193916349809886%\n",
      "      Team 1 Score  Predicted Team 1 Score  Team 2 Score  \\\n",
      "0               92                   67.22            84   \n",
      "1               80                   80.39            51   \n",
      "2               84                   91.81            71   \n",
      "3               79                   81.49            73   \n",
      "4               76                   94.44            74   \n",
      "...            ...                     ...           ...   \n",
      "1310            88                     NaN            81   \n",
      "1311            57                     NaN            56   \n",
      "1312            72                     NaN            59   \n",
      "1313            72                     NaN            71   \n",
      "1314            76                     NaN            59   \n",
      "\n",
      "      Predicted Team 2 Score  Team 1 Error  Team 2 Error  Team 1 Accurate  \\\n",
      "0                      58.02         24.78         25.98            False   \n",
      "1                      72.99          0.39         21.99             True   \n",
      "2                      71.47          7.81          0.47             True   \n",
      "3                      67.06          2.49          5.94             True   \n",
      "4                      73.31         18.44          0.69            False   \n",
      "...                      ...           ...           ...              ...   \n",
      "1310                     NaN           NaN           NaN            False   \n",
      "1311                     NaN           NaN           NaN            False   \n",
      "1312                     NaN           NaN           NaN            False   \n",
      "1313                     NaN           NaN           NaN            False   \n",
      "1314                     NaN           NaN           NaN            False   \n",
      "\n",
      "      Team 2 Accurate  \n",
      "0               False  \n",
      "1               False  \n",
      "2                True  \n",
      "3                True  \n",
      "4                True  \n",
      "...               ...  \n",
      "1310            False  \n",
      "1311            False  \n",
      "1312            False  \n",
      "1313            False  \n",
      "1314            False  \n",
      "\n",
      "[1315 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "newpath = 'predicted.csv'  # Update with your actual file path\n",
    "readpath = pd.read_csv(newpath)\n",
    "\n",
    "\n",
    "# Placeholder - you need actual scores for this part\n",
    "# predictions_with_teams['Actual Team 1 Score'] = [actual_scores_from_somewhere]\n",
    "# predictions_with_teams['Actual Team 2 Score'] = [actual_scores_from_somewhere]\n",
    "\n",
    "# Let's say we define a correct prediction as being within 10 points of the actual score\n",
    "# Calculate the error for each team's score prediction\n",
    "readpath['Team 1 Error'] = abs(readpath['Predicted Team 1 Score'] - readpath['Team 1 Score'])\n",
    "readpath['Team 2 Error'] = abs(readpath['Predicted Team 2 Score'] - readpath['Team 2 Score'])\n",
    "\n",
    "# Define a correct prediction as having an error of 10 points or fewer\n",
    "accuracy_threshold = 10\n",
    "readpath['Team 1 Accurate'] = readpath['Team 1 Error'] <= accuracy_threshold\n",
    "readpath['Team 2 Accurate'] = readpath['Team 2 Error'] <= accuracy_threshold\n",
    "\n",
    "# Calculate the percentage of accurate predictions\n",
    "team_1_accuracy_percentage = readpath['Team 1 Accurate'].mean() * 100\n",
    "team_2_accuracy_percentage = readpath['Team 2 Accurate'].mean() * 100\n",
    "\n",
    "print(f\"Team 1 Prediction Accuracy: {team_1_accuracy_percentage}%\")\n",
    "print(f\"Team 2 Prediction Accuracy: {team_2_accuracy_percentage}%\")\n",
    "\n",
    "print(readpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
